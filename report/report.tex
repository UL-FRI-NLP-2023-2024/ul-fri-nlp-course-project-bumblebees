%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
%
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com)
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}
\usepackage{amsmath}
\graphicspath{{fig/}}

\usepackage{multirow}
\usepackage{makecell}
\usepackage{hhline}

\usepackage{color, colortbl}
\definecolor{Blue}{rgb}{0.8549,0.9098,0.9882}
\definecolor{Red}{rgb}{0.9725,0.8078,0.8}


%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2024}

% Interim or final report
\Archive{Project report}
%\Archive{Final report}

% Article title
\PaperTitle{Unsupervised Domain adaptation for Sentence Classification}

% Authors (student competitors) and their info
\Authors{Veronika Matek, Karmen Frank and Luka Mihelič}

% Advisors
\affiliation{\textit{Advisors: Boshko Koloski, Aleš Žagar, Slavko Žitnik}}

% Keywords
\Keywords{Domain Adaptation, TSDAE, GPL, SBERT, Sentence Classification}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
Methods like SBERT are used for deriving sentence embeddings, but often do not perform well on more specific domains. This problem might be avoided if the chosen base model is fine-tuned using unsupervised fine-tuning methods like TSDAE and GPL.
}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom

% Print the title and abstract box
\maketitle

% Removes page numbering from the first page
\thispagestyle{empty}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}

\iffalse
	Uvodni zapis - slabosti SBERT-a in podobnih metod, zakaj se uporablja TSDAE in GPL (v povezavi s prilagoditvijo domene). Osredotočili se bomo na "domain adaptation" za slovenski jezik, pri čemer bomo metode testirali nad problemom "sentimental analysis" (nekako povežemo besedilo na problem klasifikacija pri čustvih). Cilj projekta je bila implementacija metod TSDAE, GPL, s katerimi finetunamo SBERT (BERT+pooling), in testiranje metod ter primerjava rezultatov nad problemom klasifikacije stavkov v eno izmed "pos", "neg", "neutral". Opazovali smo tudi izbrane parametre tekom učenja posameznega pristopa.

	In the Introduction section you should write about the
	relevance of your work (what is the purpose of the project,
	what will we solve) and about related work (what solutions
	for the problem already exist). Where appropriate, reference
	scientific work conducted by other researchers.
\fi

Previous state-of-the-art methods, like SBERT, for deriving sentence embeddings have a key problem of not working for specific topics and domains. We bypass this problem by additionally fine-tuning our non-domain-specific base model using methods like TSDAE and GPL. Both of the methods have been proven to significantly outperform previous state-of-the-art models, like Masked Language Model, on domain specific training data, working even better if combined together~\cite{wang-etal-2021-tsdae-using, GPL}.

Most of these previous successful methods were trained on Semantic Textual Similarity, which does not take into account any domain knowledge. Some examples of these approaches are SBERT and Infersent. One such reason for the lack of domain knowledge is that training a model might require a lot of labeled data, which can be expensive and hard to get. This holds true especially for specific topics. One way of solving this is training the model on the general corpus before fine-tuning it to the required domain~\cite{wang-etal-2021-tsdae-using, GPL}.

In this report we aim to fine-tune an unsupervised multilingual base model with SBERT architecture with two of the mentioned domain adaptation techniques, namely Transformer-based Sequential Denoising Auto-Encoder (TSDAE) and Generative Pseudo Labeling (GPL) on the SentiNews classification dataset. We classify Slovenian sentences based on their sentiment, which can be positive, neutral or negative. We compare the results given by the pretrained base model and by the base model fine-tuned with both mentioned methods and combinations of the two.

We observe any improvement when faced with domain specific data compared to SBERT, trained on the exact same input sentences. We also distinguish the impact of different parameters during the learning of each approach and try to find the optimums.



%------------------------------------------------



\section*{Methods}

\iffalse
Use the Methods section to describe what you did an how you did it -- in what way did you prepare the data, what algorithms did you use, how did you test various solutions ... Provide all the required details for a reproduction of your work.
\fi

\subsection*{SBERT}
SBERT (Sentence Bidirectional Encoder Representations from Transformers) adds siamese and triplet structure networks to the pre-trained transformer network BERT, which produces state-of-the-art results for natural language processing tasks such as sentence classification, question answering and sentence-pair regression. SBERT applies a pooling layer to the output of a BERT/RoBERTa model, deriving fixed sized sentence embeddings. With the added network structures we can fine-tune the model and update weights so the output results are sentence embeddings that are semantically meaningful~\cite{SBERT}.

Semantic aspects embedded in the continuous vector space can be measured with the cosine metric similarity, where similar semantic representations in a high-dimensional vector space are closer to each other. The available training data for a given knowledge domain also defines the SBERT network structure. Therefore we may use the classification, regression or the triplet objective function for different kind of tasks.

The classification objective function concatenates sentence embeddings \(u\) and \(v\) with the element-wise difference \(|u - v|\) and multiplies it with the trainable weight \( W_t \in \mathbb{R}^{3n \times k} \) \cite{SBERT}:

\begin{equation}
\begin{gathered}
o = \text{softmax}(W_t(u, v, |u - v|)) \text{,}
\end{gathered}
\label{eq:softmax}
\end{equation}

where $n$ is the sentence embeddings' dimension and $k$ number of labels.


\subsection*{TSDAE}

Transformer-based Sequential Denoising Auto-Encoder (TSDAE) is a state-of-the-art unsupervised method used for domain adaptation with an encoder-decoder architecture. A shortcoming of previous sentence embedding techniques like SBERT is the lack of domain knowledge. Fine-tuning a model like this with TSDAE can adapt our model to a specific domain without any labeled data, as this is hard and expensive to acquire~\cite{wang-etal-2021-tsdae-using}.

Before training the model, TSDAE corrupts the input sentences, for example by deleting or swapping words, and encodes them to a fixed size vector. The goal of the decoder is to reconstruct the vectors of the original input by prediciting what was changed. It is important to note that the decoder has no context as it doesn't have access to other sentence embeddings and thus creates a bottleneck~\cite{wang-etal-2021-tsdae-using}. This architecture can be seen in Figure~\ref{fig:tsdae}.

\begin{figure}[ht]\centering
	\vspace{12 pt}
	\includegraphics[width=\linewidth]{TSDAE_scheme.pdf}
	\vspace{5 pt}
	\caption{Workflow of TSDAE. The input sentences are first corrupted and then encoded into fixed size vectors. The vectors are pooled and then attempted to be reconstructed with the decoder.}
	\label{fig:tsdae}
\end{figure}

For the purpose of classifying Slovenian sentences based on their sentiment we fine-tune the SBERT model with TSDAE. We choose \textit{paraphrase-multilingual-MiniLM-L12-v2} for our base model. During training we use the DenoisingAutoEncoderLoss as our loss function, which expects pairs of original and corrupted sentences as the input. We train the model where the decoder attempts the reconstruction of the corrupted sentences and compare our results with the corpus~\cite{SentenceTransformers}.

% TSDAE has been shown by Wang, Reimers and Gurevych \cite{wang-etal-2021-tsdae-using} to outperform other unsupervised approaches and other supervised models, trained with a lot of labeled data. Many previous works were evaluated on Semantic Textual Similarity (STS) which might return good performance but it is unclear how it performs on specific domains \cite{wang-etal-2021-tsdae-using}.


\subsection*{GPL}

The Generative Pseudo Labeling (GPL) is a domain adaptation technique that utilizes unsupervised learning. It allows us to fine-tune a dense retrieval model (in our case SBERT \cite{SBERT}) on a desired domain. First step of GPL is preparing (query, sentence)-pairs. This takes three phases: generating suitable queries, negative mining and using cross-encoder to assign a score to each pair \cite{GPL}. This process is visualised in Figure~\ref{fig:GPL}.

\begin{figure}[ht]\centering
	\vspace{12 pt}
	\includegraphics[width=\linewidth]{GPL_data_preprocessing.pdf}
	\vspace{5 pt}
	\caption{The workflow of GPL's sentence preparation step. Queries $Q$ are generated for each input sentence $P^{+}$. The generated queries are then used for negative mining or finding similar sentences $P^{-}$. Pseudo labeling step involves a cross-encoder that assigns a score to each (query, sentence)-pair.}
	\label{fig:GPL}
\end{figure}

Queries are generated using a pretrained T5 encoder-decoder model \cite{T5}. Three queries are generated for each input sentence. The next step is negative mining, where 50 of the most similar sentences are retrieved for each of the generated queries, using an existing dense retrieval model. The (query, input sentence)-pairs are denoted as $(Q, P^{+})$ and the negative sentence as $P^{-}$.

The last step of data preparation involves a cross-encoder that assigns a score to each (query, sentence)-pair. For each $(Q, P^{+}, P^{-})$-tuple a margin $\delta$ is calculated using the next equation:
\begin{equation}
	\delta = \text{CE}(Q, P^{+}) - \text{CE}(Q, P^{-})\text{,}
\label{eq:delta}
\end{equation}
where $CE$ is the score predicted by the cross-encoder. This gives us a dataset $D_{GPL} = {\{ ( Q_i, P_i, P_i^{-}, \delta_i ) \}}_i$, which is used for training a dense retrieval model with the MarginMSE loss function. This model thus learns to map queries and sentences into a vector space and is fine-tuned to a given domain.

The MarginMSE loss \cite{marginMSE} relies on the scores, or pseudo labels, provided by the cross-encoder. It teaches the dense retrieval model to predict the margin between the score of $(Q, P^{+})$-pair and score of $(Q, P^{-})$-pair. It follows the next equation:
\begin{equation}
	\text{MarginMSE} = \frac{1}{N} \sum_{i=0}^{N-1} |\hat{\delta_i} - \delta_i|^{2} \text{,}
\label{eq:margin}
\end{equation}
where $N$ is the batch size, $\delta_i$ is defined in equation~\ref{eq:delta}, provided by the cross-encoder, and $\hat{\delta_i}$ is derived by the (student) dense retrieval model, which we are fine-tuning.


%------------------------------------------------



\section*{Experiments}
The experiments were conducted on two different sentence-transformer (SBERT architecture) base models, the first one being multilingual {\it paraphrase-multilingual-MiniLM-L12-v2}~\cite{reimers-2019-sentence-bert} and the second Slovenian {\it SloBERTa}~\cite{sloberta}. Both models were individually fine-tuned with the TSDAE and GPL method and combinations TSDAE+GPL and GPL+TSDAE.

We used the SentiNews dataset \cite{sentiNews}, which contains 169k sentences from 10.4k documents, equiped with sentiment labels, in the Slovenian language. The sentiment labels can be neutral, negative or positive. A few examples of dataset's elements are shown in Table~\ref{tab1}.

\begin{table}[!h]
	\footnotesize
	\begin{center}
		\begin{tabular}{ |l|c| }
		\hline
		\rowcolor{Blue}Sentence & Sentiment \\
		\hline

		Kaže, da se blejskim vilam vendarle obeta lepša prihodnost. & positive\\
		O tem bo Evropska komisija odločala septembra. & neutral\\
		V Sloveniji je ta rast znašala sedem odstotkov. & negative\\

		\hline
		\end{tabular}
	\end{center}
\caption{Examples from the SentiNews dataset \cite{sentiNews}.}
\label{tab1}
\end{table}

The dataset was split into train (70\%), validation (10\%) and test set (20\%). For each method we use to fine-tune the base model, the exact same datasets are used.

The approaches were tested on the test set and evaluated using the F1 score that combines precision and recall. It is defined by the following equation:
\begin{equation}
	\text{F1 score} =\frac{2 \times \text{\it{precision}} \times \text{\it{recall}}}{\text{\it{precision}} + \text{\it{recall}}} \text{,}
\label{eq:f1}
\end{equation}
where precision and recall are defined as:
\begin{equation}
	\text{precision} =\frac {\text{TP}}{\text{TP} + \text{FP}} \text{,}
\label{eq:precision}
\end{equation}
\begin{equation}
	\text{recall} =\frac {\text{TP}}{\text{TP} + \text{FN}} \text{.}
\label{eq:recall}
\end{equation}

Precision metric of the model tells us how many accurate positive predictions have been made based on the equation \ref{eq:precision} and recall how well the model covered all true positives from the positive sample set as shown in equation \ref{eq:recall}. The F1 score is a relative metric that tells us how well our chosen model extracts sentiment polarity from semantics and shows overall performance for each possible sentiment class triplet.



\subsection*{Classifier}

To be able to test the base model and its fine-tuned variants on the sentiment classification problem, we prepared a simple classifier. The classifier takes a sentence encoding of length 384 (or 768 if the base model is {\it SloBERTa}) and transforms it into a vector of size 3 - the number of classes in our classification problem. The transformation is done via a trainable linear layer. Softmax is used on this vector to obtain probabilities for each of the classes and as the output of the classifier model the class with the highest probability is returned.

For each model that we evaluated, a new classifier was trained. All of the classifiers were trained on the same training set and used the same validation set. Data was split into batches of size 32, we used learning rate 0.001 with the Adam optimizer and trained the classifier for 10 epochs. The classifier model with the highest F1 score on the validation set was chosen.


\subsection*{TSDAE}

For TSDAE fine-tuning we first prepare our dataset split and then select our base model.
We choose a sentence-transformer model {\it paraphrase-multilingual-MiniLM-L12-v2} as our starting point of training, which maps sentences to 384 dimensional vectors or sentence encodings. The second base model SloBERTa was also fine-tuned using this approach.

We take the training data of our split dataset and corrupt the sentences by removing words using the {\it DenoisingAutoEncoderDataset}. This function is used in combination with {\it DenoisingAutoEncoderLoss} which tries to reconstruct the sentences without noise.

We train the model using batch size 32, 10 epochs and learning rate of 3e-5. We then train a classifier to predict sentiment from the 384-dimensional vectors (or 768 for SloBERTa). We calculate the precision, recall and F1 score on the test dataset.


\subsection*{GPL}
The fine-tuning with the GPL method consisted of training two versions. Each one was fine-tuned using a different T5 model for query qeneration. The training dataset of the first T5 model did not include the Slovene language, this model is also known as {\it msmarco-14langs-mt5-base-v1}~\cite{msmarco14langs}. The second choice for the T5 model is made specifically for the Slovene language and producing Slovene queries, this is the {\it slv\_doc2query}~\cite{boshko} transformer model.

We train the model for 140 000 steps, with the batch size set to 32, generating 3 queries per sentence and 50 negative examples per query. For the score evaluation method for dense vector retrieval we use computation of cosine similarity, which also works for usage of the default hybrid retriever in combination of {\it msmarco-distilbert-base-v3 }~\cite{msmarcodistil} and {\it msmarco-MiniLM-L-6-v3}~\cite{msmarcominilm} for negative mining. After fine-tuning the chosen base model with the GPL, we train a classifier that receives as input 384-dimensional vector embedding (or 768 in the case of SloBERTa) and returns the sentiment classification. The results are evaluated in the same way as the base models and TSDAE fine-tuned models.


%------------------------------------------------



\section*{Results}

% TO DO:
% Use the results section to present the final results of your work. Present the results in a objective and scientific fashion. Use visualisations to convey your results in a clear and efficient manner. When comparing results between various techniques use appropriate statistical methodology.

\subsection*{\large{1. Paraphrase-multilingual-MiniLM}}

The fine-tuning and evaluation was first executed on the {\it paraphrase-multilingual-MiniLM-L12-v2}~\cite{reimers-2019-sentence-bert} model. The compared models in this section were:
\begin{itemize}
	\item base model {\it paraphrase-multilingual-MiniLM-L12-v2},
	\item base model fine-tuned with the TSDAE method, trained for 5 epochs - denoted as $\text{TSDAE}^{5}$,
	\item base model fine-tuned with the TSDAE method, trained for 10 epochs - denoted as TSDAE,
	\item base model fine-tuned with the TSDAE method, trained for 15 epochs - denoted as $\text{TSDAE}^{15}$,
	\item base model fine-tuned with the GPL method and {\it msmarco-14langs-mt5-base-v1} \cite{msmarco14langs} for the T5 model - denoted as GPL and
	\item base model fine-tuned with the GPL method and {\it slv\_doc2query}~\cite{boshko} for the T5 model - denoted as $\text{GPL}_{SLO}$.
\end{itemize}

A new classifier was trained with the same parameters for each of the stated models. They were evaluated on the test set part of the SentiNews dataset. The results are presented in Table~\ref{tab2}.

\begin{table}[!h]
	\footnotesize
	\begin{center}
		\begin{tabular}{ |l|c|c|c| }
		\hline
		\rowcolor{Blue}Model & Precision & Recall & F1 score\\
		%\hhline{|=|=|=|=|}
		\hline

		Base model & 0.v & 0.v & 0.v\\
		\hline
		$\text{TSDAE}^{5}$ & 0.v & 0.v & 0.v\\
		TSDAE & 0.v & 0.v & 0.v\\
		$\text{TSDAE}^{15}$ & 0.v & 0.v & 0.v\\
		\hline
		GPL & 0.v & 0.v & 0.v\\
		$\text{GPL}_{SLO}$ & 0.v & 0.v & 0.v\\

		\hline
		\end{tabular}
	\end{center}
\caption{Results of the base model and its fine-tuned variants on the SentiNews dataset~\cite{sentiNews}.}
\label{tab2}
\end{table}

% REZULTATE MORAMO ŠE POKOMENTIRATI


\subsection*{Combining the methods}
Considering results obtained in \cite{GPL}, we decided to test two combinations of the unsupervised adaptation techniques. The approaches were:
\begin{itemize}
	\item additionally fine-tune the TSDAE model with the GPL method with {\it slv\_doc2query} for the T5 model,
	\item additionally fine-tune the $\text{GPL}_{SLO}$ model with TSDAE method for 10 epochs.
\end{itemize}
Results, compared to the base model, are presented in Table~\ref{tab3}.

\begin{table}[!h]
	\footnotesize
	\begin{center}
		\begin{tabular}{ |l|c|c|c| }
		\hline
		\rowcolor{Blue}Model & Precision & Recall & F1 score\\
		\hline

		Base model & 0.v & 0.v & 0.v\\
		TSDAE+$\text{GPL}_{SLO}$ & 0.v & 0.v & 0.v\\
		$\text{GPL}_{SLO}$+TSDAE & 0.v & 0.v & 0.v\\

		\hline
		\end{tabular}
	\end{center}
\caption{Results of combining the fine-tuning methods TSDAE and GPL in different orders on the SentiNews dataset~\cite{sentiNews}.}
\label{tab3}
\end{table}

% REZULTATE MORAMO ŠE POKOMENTIRATI


\subsection*{Intermediate models in GPL}
Since we used the \textit{gpl} library for fine-tuning a chosen model with the GPL method, it allowed us to save intermediate models during this process. The GPL method fine-tunes the model for 140 000 steps by default and saves a model every 10 000 steps. Thus we obtained 13 intermediate and 1 final model. We trained a classifier for each of the models and evaluated them on the test set part of the SentiNews dataset~\cite{sentiNews}. The results were plotted and are shown in Figure~\ref{fig:gpl-versions} for GPL and in Figure~\ref{fig:gpl-slo-versions} for $\text{GPL}_{SLO}$.

\begin{figure}[ht]\centering
	\vspace{12 pt}
	\includegraphics[width=\linewidth]{GPL_14langs_paraphrase.png}
	\vspace{5 pt}
	\caption{Visualisation of the F1 score evaluated on models, obtained during fine-tuning with the GPL method with {\it msmarco-14langs-mt5-base-v1} \cite{msmarco14langs} for the T5 model.}
	\label{fig:gpl-versions}
\end{figure}

\begin{figure}[ht]\centering
	\vspace{12 pt}
	\includegraphics[width=\linewidth]{GPL_Boshko_paraphrase.png}
	\vspace{5 pt}
	\caption{Visualisation of the F1 score evaluated on models, obtained during fine-tuning with the GPL method with {\it slv\_doc2query}~\cite{boshko} for the T5 model.}
	\label{fig:gpl-slo-versions}
\end{figure}

% REZULTATE MORAMO ŠE POKOMENTIRATI


\subsection*{\large{2. SloBERTa}}
Spremenjen base model, treniramo še ta boljši TSDAE in GPL od prej.
Primerjamo tudi vrednosti iz te tabele in iz tabele 2.

GPL SLO T5 je uporabljen!
TSADE x epoh

\begin{table}[!h]
	\footnotesize
	\begin{center}
		\begin{tabular}{ |l|c|c|c| }
		\hline
		\rowcolor{Blue}Model & Precision & Recall & F1 score\\
		\hline

		Base model & 0.v & 0.v & 0.v\\
		%\hline
		TSDAE & 0.v & 0.v & 0.v\\
		%\hline
		%GPL & 0.v & 0.v & 0.v\\
		$\text{GPL}_{SLO}$ & 0.v & 0.v & 0.v\\

		\hline
		\end{tabular}
	\end{center}
\caption{Results of the base model and its fine-tuned variants on the SentiNews dataset~\cite{sentiNews}.}
\label{tab4}
\end{table}

% REZULTATE MORAMO ŠE POKOMENTIRATI


\subsection*{Intermediate models in GPL}
Intermediate:

\begin{figure}[ht]\centering
	\vspace{12 pt}
	\includegraphics[width=\linewidth]{GPL_Boshko_SloBERTa.png}
	\vspace{5 pt}
	\caption{Visualisation of the F1 score evaluated on models, obtained during fine-tuning with the GPL method with {\it slv\_doc2query}~\cite{boshko} for the T5 model.}
	\label{fig:gpl-slo-sloberta-versions}
\end{figure}


% REZULTATE MORAMO ŠE POKOMENTIRATI



%------------------------------------------------



\section*{Discussion}

TO DO:
Use the Discussion section to objectively evaluate your work, do not just put praise on everything you did, be critical and exposes flaws and weaknesses of your solution. You can also explain what you would do differently if you would be able to start again and what upgrades could be done on the project in the future.



%------------------------------------------------



\section*{Acknowledgments}

Here you can thank other persons (advisors, colleagues ...) that contributed to the successful completion of your project.


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}


\end{document}
