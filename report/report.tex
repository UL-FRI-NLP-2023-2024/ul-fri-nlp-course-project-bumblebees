%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
%
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com)
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}
\usepackage{amsmath}
\graphicspath{{fig/}}

\usepackage{multirow}
\usepackage{makecell}

\usepackage{color, colortbl}
\definecolor{Blue}{rgb}{0.8549,0.9098,0.9882}
\definecolor{Red}{rgb}{0.9725,0.8078,0.8}


%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2024}

% Interim or final report
\Archive{Project report}
%\Archive{Final report}

% Article title
\PaperTitle{Unsupervised Domain adaptation for Sentence Classification}

% Authors (student competitors) and their info
\Authors{Veronika Matek, Karmen Frank and Luka Mihelič}

% Advisors
\affiliation{\textit{Advisors: Slavko Žitnik, Aleš Žagar, Boshko Koloski}}

% Keywords
\Keywords{Domain Adaptation, TSDAE, GPL, SBERT, Sentence Classification}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
TO BE ADDED LATER IN PROCESS.
}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom

% Print the title and abstract box
\maketitle

% Removes page numbering from the first page
\thispagestyle{empty}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}

\iffalse
	Uvodni zapis - slabosti SBERT-a in podobnih metod, zakaj se uporablja TSDAE in GPL (v povezavi s prilagoditvijo domene). Osredotočili se bomo na "domain adaptation" za slovenski jezik, pri čemer bomo metode testirali nad problemom "sentimental analysis" (nekako povežemo besedilo na problem klasifikacija pri čustvih). Cilj projekta je bila implementacija metod TSDAE, GPL, s katerimi finetunamo SBERT (BERT+pooling), in testiranje metod ter primerjava rezultatov nad problemom klasifikacije stavkov v eno izmed "pos", "neg", "neutral". Opazovali smo tudi izbrane parametre tekom učenja posameznega pristopa.

	In the Introduction section you should write about the
	relevance of your work (what is the purpose of the project,
	what will we solve) and about related work (what solutions
	for the problem already exist). Where appropriate, reference
	scientific work conducted by other researchers.
\fi

Previous state-of-the-art methods, like SBERT, for deriving sentence embeddings have a key problem of not working for specific topics and domains. We bypass this problem by additionally fine-tuning our non-domain-specific base model using methods like TSDAE and GPL. Both of the methods have been proven to significantly outperform previous state-of-the-art models, like Masked Language Model, on domain specific training data, working even better if combined together~\cite{wang-etal-2021-tsdae-using, GPL}.

Most of these previous successful methods were trained on Semantic Textual Similarity, which does not take into account any domain knowledge. Some examples of these approaches are SBERT and Infersent. One such reason for the lack of domain knowledge is that training a model might require a lot of labeled data, which can be expensive and hard to get. This holds true especially for specific topics. One way of solving this is training the model on the general corpus before fine-tuning it to the required domain~\cite{wang-etal-2021-tsdae-using, GPL}.

In this report we aim to fine-tune an unsupervised multilingual base model SBERT with two of the mentioned domain adaptation techniques, namely Transformer-based Sequential Denoising Auto-Encoder (TSDAE) and Generative Pseudo Labeling (GPL) on the SentiNews classification dataset. We classify Slovenian sentences based on their sentiment, which can be positive, neutral or negative. We compare the results given by the pretrained base model and by the base model fine-tuned with both mentioned methods.

We observe any improvement when faced with domain specific data compared to SBERT, trained on the exact same input sentences. We also distinguish the impact of different parameters during the learning of each approach and try to find the optimums.



%------------------------------------------------



\section*{Methods}

\iffalse
Use the Methods section to describe what you did an how you did it -- in what way did you prepare the data, what algorithms did you use, how did you test various solutions ... Provide all the required details for a reproduction of your work.
\fi

\subsection*{SBERT}
SBERT (Sentence Bidirectional Encoder Representations from Transformers) adds siamese and triplet structure networks to the pre-trained transformer network BERT, which produces state-of-the-art results for natural language processing tasks such as question answering, sentence classification and sentence-pair regression. SBERT applies a pooling layer to the output of a BERT/RoBERTa model, deriving fixed sized sentence embeddings. With the added network structures we can fine-tune the model and update weights so the to output results are sentence embeddings that are semantically meaningful \cite{SBERT}. Semantic aspects embedded in the continuous vector space can be measured with the cosine metric similarity, where similar semantic representations in a high-dimensional vector space are closer to each other. The available training data for a given knowledge domain also defines the SBERT network structure. Therefore we may use the classification, regression or the triplet objective function for different kind of tasks.

The classification objective function concatenates sentence embeddings \(u\) and \(v\) with the element-wise difference \(|u - v|\) and multiplies it with the trainable weight \( W_t \in \mathbb{R}^{3n \times k} \) \cite{SBERT}:

\begin{equation}
\begin{gathered}
o = \text{softmax}(W_t(u, v, |u - v|)) \text{,}
\end{gathered}
\label{eq:softmax}
\end{equation}

where $n$ is the sentence embeddings' dimension and $k$ number of labels.


\subsection*{TSDAE}

Transformer-based Sequential Denoising Auto-Encoder (TSDAE) is a state-of-the-art unsupervised method used for domain adaptation with an encoder-decoder architecture. A shortcoming of previous sentence embedding techniques like SBERT is the lack of domain knowledge. Fine-tuning a model like this with TSDAE can adapt our model to a specific domain without any labeled data, as this is hard and expensive to acquire~\cite{wang-etal-2021-tsdae-using}.

Before training the model, TSDAE corrupts the input sentences, for example by deleting or swapping words, and encodes them to a fixed size vector. The goal of the decoder is to reconstruct the vectors of the original input by prediciting what was changed. It is important to note that the decoder has no context as it doesn't have access to other sentence embeddings and thus creates a bottleneck~\cite{wang-etal-2021-tsdae-using}. This architecture can be seen in Figure~\ref{fig:tsdae}.

\begin{figure}[ht]\centering
	\vspace{12 pt}
	\includegraphics[width=\linewidth]{TSDAE_scheme.pdf}
	\vspace{5 pt}
	\caption{Workflow of TSDAE. The input sentences are first corrupted and then encoded into fixed size vectors. The vectors are pooled and then attempted to be reconstructed with the decoder.}
	\label{fig:tsdae}
\end{figure}

For the purpose of classifying Slovenian sentences based on their sentiment we fine-tune the SBERT model with TSDAE. We choose \textit{paraphrase-multilingual-MiniLM-L12-v2} for our base model. During training we use the DenoisingAutoEncoderLoss as our loss function, which expects pairs of original and corrupted sentences as the input. We train the model where the decoder attempts the reconstruction of the corrupted sentences and compare our results with the corpus~\cite{SentenceTransformers}.

% TSDAE has been shown by Wang, Reimers and Gurevych \cite{wang-etal-2021-tsdae-using} to outperform other unsupervised approaches and other supervised models, trained with a lot of labeled data. Many previous works were evaluated on Semantic Textual Similarity (STS) which might return good performance but it is unclear how it performs on specific domains \cite{wang-etal-2021-tsdae-using}.


\subsection*{GPL}

The Generative Pseudo Labeling (GPL) is a domain adaptation technique that utilizes unsupervised learning. It allows us to fine-tune a dense retrieval model (in our case SBERT \cite{SBERT}) on a desired domain. First step of GPL is preparing (query, sentence)-pairs. This takes three phases: generating suitable queries, negative mining and using cross-encoder to assign a score to each pair \cite{GPL}. This process is visualised in Figure~\ref{fig:GPL}.

\begin{figure}[ht]\centering
	\vspace{12 pt}
	\includegraphics[width=\linewidth]{GPL_data_preprocessing.pdf}
	\vspace{5 pt}
	\caption{The workflow of GPL's sentence preparation step. Queries $Q$ are generated for each input sentence $P^{+}$. The generated queries are then used for negative mining or finding similar sentences $P^{-}$. Pseudo labeling step involves a cross-encoder that assigns a score to each (query, sentence)-pair.}
	\label{fig:GPL}
\end{figure}

Queries are generated using a pretrained T5 encoder-decoder model \cite{T5}. Three queries are generated for each input sentence. The next step is negative mining, where 50 of the most similar sentences are retrieved for each of the generated queries, using an existing dense retrieval model. The (query, input sentence)-pairs are denoted as $(Q, P^{+})$ and the negative sentence as $P^{-}$.

The last step of data preparation involves a cross-encoder that assigns a score to each (query, sentence)-pair. For each $(Q, P^{+}, P^{-})$-tuple a margin $\delta$ is calculated using the next equation:
\begin{equation}
	\delta = \text{CE}(Q, P^{+}) - \text{CE}(Q, P^{-})\text{,}
\label{eq:delta}
\end{equation}
where $CE$ is the score predicted by the cross-encoder. This gives us a dataset $D_{GPL} = {\{ ( Q_i, P_i, P_i^{-}, \delta_i ) \}}_i$, which is used for training a dense retrieval model with the MarginMSE loss function. This model thus learns to map queries and sentences into a vector space and is fine-tuned to a given domain.

The MarginMSE loss \cite{marginMSE} relies on the scores, or pseudo labels, provided by the cross-encoder. It teaches the dense retrieval model to predict the margin between the score of $(Q, P^{+})$-pair and score of $(Q, P^{-})$-pair. It follows the next equation:
\begin{equation}
	\text{MarginMSE} = \frac{1}{N} \sum_{i=0}^{N-1} |\hat{\delta_i} - \delta_i|^{2} \text{,}
\label{eq:margin}
\end{equation}
where $N$ is the batch size, $\delta_i$ is defined in equation~\ref{eq:delta}, provided by the cross-encoder, and $\hat{\delta_i}$ is derived by the (student) dense retrieval model, which we are fine-tuning.


%------------------------------------------------



\section*{Experiments}
For the base model {\it paraphrase-multilingual-MiniLM-L12-v2}~\cite{reimers-2019-sentence-bert} was chosen.
% tu je treba še kaj povedati o tem modelu, morda kaj vse smo testirali (sam base model pa klasifikator, tsdae, dva načina gplja) - in da so podrobnosti od fine-tunanih metod opisane v nadaljevanju

% \subsection*{Data}
We used the SentiNews dataset \cite{sentiNews}, which contains 169k sentences from 10.4k documents, equiped with sentiment labels, in the Slovenian language. The sentiment labels can be neutral, negative or positive. A few examples of dataset's elements are shown in Table~\ref{tab1}.

\begin{table}[!h]
	\footnotesize
	\begin{center}
		\begin{tabular}{ |l|c| }
		\hline
		\rowcolor{Red}Sentence & Sentiment \\
		\hline

		Kaže, da se blejskim vilam vendarle obeta lepša prihodnost. & positive\\
		O tem bo Evropska komisija odločala septembra. & neutral\\
		V Sloveniji je ta rast znašala sedem odstotkov. & negative\\

		\hline
		\end{tabular}
	\end{center}
\caption{Examples from the SentiNews dataset \cite{sentiNews}.}
\label{tab1}
\end{table}

The dataset was split into train (70\%), validation (10\%) and test set (20\%). For each method we use to fine-tune the base model, the exact same datasets are used.

The approaches were tested on the test set and evaluated using the F1 score shown on the following equation \ref{eq:f1}:
% tu dopišemo še formulo za F1 score, kako se računa, kaj pomeni

\begin{equation}
	\text{F1 score} =\frac{2 \times \text{\it{precision}} \times \text{\it{recall}}}{\text{\it{precision}} + \text{\it{recall}}} \text{,}
\label{eq:f1}
\end{equation}

F1 score is a metric that combines precision and recall from evaluating the performance of the model where they are defined as:

\begin{equation}
	\text{\it{precision}} =\frac {\text{TP}}{\text{TP} + \text{FP}} \text{,}
\label{eq:precision}
\end{equation}

\begin{equation}
	\text{\it{recall}} =\frac {\text{TP}}{\text{TP} + \text{FN}} \text{,}
\label{eq:recall}
\end{equation}

Precision metric of the model tells us how many accurate positive predictions have been made based on the equation \ref{eq:precision} and recall how well the model covered all true positives from the positive sample set. as shown on equation \ref{eq:recall}. The F1 score is a relative metric that tells us how well our chosen transformer model in the GPL adaptation extracts sentiment polarity from semantics and shows overall performance for each possible sentiment class triplet.

% \subsection*{Testing approach}
% Naslov morda še ni ustrezen in se bo prilagodil.
% Katero metriko uporabimo za primerjavo rezultatov, kako iz sentence embedding pridemo do klasifikacije povedi.


\subsection*{Classifer}

To be able to test the base model and its fine-tuned variants on the sentiment classification problem, we prepared a simple classifier. The classifier takes a sentence encoding of length 384 and transforms it into a vector of size 3 - the number of classes in our classification problem. The transformation is done via a trainable linear layer. Softmax is used on this vector to obtain probabilities for each of the classes and as the output of the classifier model the class with the highest probability is returned.

For each model that we evaluated, a new classifier was trained. All of the classifiers were trained on the same training set and used the same validation set. Data was split into batches of size 32, we used learning rate 0.001 with the Adam optimizer and trained the classifier for 10 epochs. The classifier model with the highest F1 score on the validation set was chosen.


\subsection*{TSDAE}

For TSDAE fine-tuning we first prepare our dataset split and then select our base model.
We choose a sentence-transformers model {\it paraphrase-multilingual-MiniLM-L12-v2}~\cite{reimers-2019-sentence-bert} as our starting point of training,
which maps sentences to 384 dimensional vectors or sentence encodings.

We take the training data of our split dataset and corrupt the sentences by removing words using the {\it DenoisingAutoEncoderDataset}. This function is used in combination with {\it DenoisingAutoEncoderLoss} which tries to reconstruct the sentences without noise.

We train the model using batch size 32, 5 epochs and learning rate of 3e-5. We then train a classifier to predict sentiment from the 384 dimensional vectors. We calculate the precision, recall, accuracy and F1 score shown in Table~\ref{tab2}.

\subsection*{GPL}
For training our GPL model we first trained two models with different T5 doc2query approaches for query qeneration. For creating a model without Slovene language recognition we use the docT5query model, also known as {\it msmarco-14langs-mt5-base-v1} \cite{msmarco14langs}, and specifically for learning Slovene language we use the {\it slv\_doc2query} \cite{boshko} transformer model. 

We train the model with the parameters set as batch size to 32, 140 000 steps, 50 negatives per query and for the score evaluation method for dense vector retrieval we use computation of cosine similarity, which also works for usage of the default hybrid retriever in combination of {\it msmarco-distilbert-base-v3 } \cite{msmarcodistil} and {\it msmarco-MiniLM-L-6-v3} \cite{msmarcominilm} for negative mining. After fine-tuning with the GPL, we train the classifier that to receive 384 vector embedding from encoded data for evaluation, similar to our TSDAE.


%------------------------------------------------



\section*{Results}

% TO DO:
% Use the results section to present the final results of your work. Present the results in a objective and scientific fashion. Use visualisations to convey your results in a clear and efficient manner. When comparing results between various techniques use appropriate statistical methodology.

The base model {\it paraphrase-multilingual-MiniLM-L12-v2}~\cite{reimers-2019-sentence-bert}, base model fine-tuned with the TSDAE method (denoted as TSDAE), base model fine-tuned with the GPL method and {\it msmarco-14langs-mt5-base-v1} \cite{msmarco14langs} for the T5 model (denoted as GPL) and base model fine-tuned with the GPL method and {\it slv\_doc2query} \cite{boshko} for the T5 model (denoted as $\text{GPL}_{SLO}$) were evaluated on the test set part of SentiNews dataset. The results are presented in Table~\ref{tab2}.

TO JE VSE NAD PARAPHRASE MODELOM!

\begin{table}[!h]
	\footnotesize
	\begin{center}
		\begin{tabular}{ |l|c|c|c|c| }
		\hline
		\rowcolor{Red}Measure & Base model & TSDAE & GPL & $\text{GPL}_{SLO}$ \\
		\hline

		precision & 0.6124 & 0.5682 & 0.4651 & 0.5641 \\
		recall & 0.6014 & 0.5598 & 0.5104 & 0.5663 \\
		F1 score & \textbf{0.5637} & 0.4968 & 0.3857 & 0.5125 \\

		\hline
		\end{tabular}
	\end{center}
\caption{Rezultati naučenih modelov nad SentiNews \cite{sentiNews} (2. oddaja).}
\label{tab2}
\end{table}


\subsection*{TSDAE}
We trained and tested base model fine-tuned with the TSDAE method for different amount of epochs.
Omenimo, da je zgoraj uporabljeni TSDAE tisti z x epohami (in damo isto ime v obeh tabelah).

\begin{table}[!h]
	\footnotesize
	\begin{center}
		\begin{tabular}{ |l|c|c|c| }
		\hline
		\rowcolor{Red}Measure & $epochs=5$ & $epochs=10$ & $epochs=15$ \\
		\hline

		precision & 0.5682 & v & v\\
		recall & 0.5598 & v & v\\
		F1 score & 0.4968 & v & v\\

		\hline
		\end{tabular}
	\end{center}
\caption{Različno št. epoh pri TSDAE.}
\label{tab3}
\end{table}


\subsection*{Combining the methods}
Kombinacija TSDAE in GPL

\begin{table}[!h]
	\footnotesize
	\begin{center}
		\begin{tabular}{ |l|c|c| }
		\hline
		\rowcolor{Red}Measure & \thead{TSDAE+\\GPL} & \thead{GPL+\\TSDAE} \\
		\hline

		precision & v & v\\
		recall & v & v\\
		F1 score & v & v\\

		\hline
		\end{tabular}
	\end{center}
\caption{Kombinacija.}
\label{tab4}
\end{table}


\subsection*{SloBERTa}
Spremenjen base model, treniramo še ta boljši TSDAE in GPL od prej.
Primerjamo tudi vrednosti iz te tabele in iz tabele 2.

GPL SLO T5 je uporabljen!
TSADE x epoh

\begin{table}[!h]
	\footnotesize
	\begin{center}
		\begin{tabular}{ |l|c|c|c| }
		\hline
		\rowcolor{Red}Measure & Base model & TSDAE & GPL \\
		\hline

		precision & v & v & 0.5914\\
		recall & v & v & 0.5854\\
		F1 score & v & v & 0.545\\

		\hline
		\end{tabular}
	\end{center}
\caption{Drug base model.}
\label{tab5}
\end{table}


%------------------------------------------------



\section*{Discussion}

TO DO:
Use the Discussion section to objectively evaluate your work, do not just put praise on everything you did, be critical and exposes flaws and weaknesses of your solution. You can also explain what you would do differently if you would be able to start again and what upgrades could be done on the project in the future.



%------------------------------------------------



\section*{Acknowledgments}

Here you can thank other persons (advisors, colleagues ...) that contributed to the successful completion of your project.


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}


\end{document}
